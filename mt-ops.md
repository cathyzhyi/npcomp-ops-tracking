# npcomp-ops-tracking

num | op | status
------------- | ------------- | -------------
1  | aten.softmax.int | -
2  | torch.aten.bmm | X 
3  | torch.aten.cumsum | -
4  | torch.aten.embedding | -
5  | aten.layer_norm | -
6  | aten.dropout | -
7  | torch.aten.slice.Tensor | -
8  | torch.aten.masked_fill.Scalar | -
9  | torch.aten.select.int | -
10 | torch.aten.index_select | -
11 | torch.aten.transpose.int | X 
12 | torch.aten.view | -
13 | torch.aten.detach | -
14 | torch.aten.mean.dim | -
15 | torch.aten.to.dtype | -
16 | torch.aten.to.other | -
17 | torch.aten.type_as | -
18 | torch.aten.contiguous | -
19 | torch.aten._shape_as_tensor | -
20 | torch.aten.arange | -
21 | torch.aten.IntImplicit | -
22 | torch.aten.Bool.Tensor | -
23 | torch.aten.add.Scalar | -
24 | torch.aten.add.Tensor | -
25 | torch.aten.copy_ | -
26 | torch.aten.cos | -
27 | torch.aten.div | -
28 | torch.aten.eq.Scalar | -
29 | torch.aten.eq.int | -
30 | torch.aten.eq.int_list | -
31 | torch.aten.exp | -
32 | torch.aten.gt.Scalar | -
33 | aten.gt.float | -
34 | aten.lt.float | -
35 | torch.aten.linear | -
36 | torch.aten.lt.int | -
37 | torch.aten.mul.Scalar | -
38 | torch.aten.mul.Tensor | -
39 | torch.aten.mul.int | -
40 | torch.aten.ne.Scalar | -
41 | torch.aten.neg.float | -
42 | torch.aten.sin | -
43 | torch.aten.size | -
44 | torch.aten.size.int | -
45 | torch.aten.tensor.int | -
46 | torch.global_slot.get | -
47 | torch.aten.format(%str_0, | -
48 | torch.prim.ListConstruct | -
49 | torch.prim.TupleConstruct | -
50 | torch.prim.TupleIndex | -
51 | torch.prim.Uninitialized | -
52 | torch.prim.device | -
53 | torch.aten.list.t | -